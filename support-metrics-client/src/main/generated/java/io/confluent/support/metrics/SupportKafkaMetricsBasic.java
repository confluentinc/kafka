/**
 * Autogenerated by Avro
 *
 * DO NOT EDIT DIRECTLY
 */
package io.confluent.support.metrics;

import org.apache.avro.generic.GenericArray;
import org.apache.avro.specific.SpecificData;
import org.apache.avro.util.Utf8;
import org.apache.avro.message.BinaryMessageEncoder;
import org.apache.avro.message.BinaryMessageDecoder;
import org.apache.avro.message.SchemaStore;

/** Represents basic metrics captured on a single Kafka broker */
@org.apache.avro.specific.AvroGenerated
public class SupportKafkaMetricsBasic extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  private static final long serialVersionUID = 6955823344478110596L;
  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"SupportKafkaMetricsBasic\",\"namespace\":\"io.confluent.support.metrics\",\"doc\":\"Represents basic metrics captured on a single Kafka broker\",\"fields\":[{\"name\":\"timestamp\",\"type\":\"long\",\"doc\":\"Time when this data record was created on the broker (Unix time).\"},{\"name\":\"kafkaVersion\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"},\"doc\":\"The version of Kafka this broker is running.\"},{\"name\":\"confluentPlatformVersion\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}],\"doc\":\"The version of the Confluent Platform this broker is running.\"},{\"name\":\"collectorState\",\"type\":\"int\",\"doc\":\"The state of the collector (e.g., Running or Shutting down).\"},{\"name\":\"brokerProcessUUID\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"},\"doc\":\"A unique identifier that is valid for the runtime of a broker.  The identifier is generated at broker startup and lost at shutdown/crash.\"},{\"name\":\"clusterId\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"},\"doc\":\"A unique identifier for the Kafka cluster.\",\"default\":\"NA\"}]}");
  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }

  private static SpecificData MODEL$ = new SpecificData();

  private static final BinaryMessageEncoder<SupportKafkaMetricsBasic> ENCODER =
      new BinaryMessageEncoder<SupportKafkaMetricsBasic>(MODEL$, SCHEMA$);

  private static final BinaryMessageDecoder<SupportKafkaMetricsBasic> DECODER =
      new BinaryMessageDecoder<SupportKafkaMetricsBasic>(MODEL$, SCHEMA$);

  /**
   * Return the BinaryMessageEncoder instance used by this class.
   * @return the message encoder used by this class
   */
  public static BinaryMessageEncoder<SupportKafkaMetricsBasic> getEncoder() {
    return ENCODER;
  }

  /**
   * Return the BinaryMessageDecoder instance used by this class.
   * @return the message decoder used by this class
   */
  public static BinaryMessageDecoder<SupportKafkaMetricsBasic> getDecoder() {
    return DECODER;
  }

  /**
   * Create a new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.
   * @param resolver a {@link SchemaStore} used to find schemas by fingerprint
   * @return a BinaryMessageDecoder instance for this class backed by the given SchemaStore
   */
  public static BinaryMessageDecoder<SupportKafkaMetricsBasic> createDecoder(SchemaStore resolver) {
    return new BinaryMessageDecoder<SupportKafkaMetricsBasic>(MODEL$, SCHEMA$, resolver);
  }

  /**
   * Serializes this SupportKafkaMetricsBasic to a ByteBuffer.
   * @return a buffer holding the serialized data for this instance
   * @throws java.io.IOException if this instance could not be serialized
   */
  public java.nio.ByteBuffer toByteBuffer() throws java.io.IOException {
    return ENCODER.encode(this);
  }

  /**
   * Deserializes a SupportKafkaMetricsBasic from a ByteBuffer.
   * @param b a byte buffer holding serialized data for an instance of this class
   * @return a SupportKafkaMetricsBasic instance decoded from the given buffer
   * @throws java.io.IOException if the given bytes could not be deserialized into an instance of this class
   */
  public static SupportKafkaMetricsBasic fromByteBuffer(
      java.nio.ByteBuffer b) throws java.io.IOException {
    return DECODER.decode(b);
  }

  /** Time when this data record was created on the broker (Unix time). */
  @Deprecated public long timestamp;
  /** The version of Kafka this broker is running. */
  @Deprecated public java.lang.String kafkaVersion;
  /** The version of the Confluent Platform this broker is running. */
  @Deprecated public java.lang.String confluentPlatformVersion;
  /** The state of the collector (e.g., Running or Shutting down). */
  @Deprecated public int collectorState;
  /** A unique identifier that is valid for the runtime of a broker.  The identifier is generated at broker startup and lost at shutdown/crash. */
  @Deprecated public java.lang.String brokerProcessUUID;
  /** A unique identifier for the Kafka cluster. */
  @Deprecated public java.lang.String clusterId;

  /**
   * Default constructor.  Note that this does not initialize fields
   * to their default values from the schema.  If that is desired then
   * one should use <code>newBuilder()</code>.
   */
  public SupportKafkaMetricsBasic() {}

  /**
   * All-args constructor.
   * @param timestamp Time when this data record was created on the broker (Unix time).
   * @param kafkaVersion The version of Kafka this broker is running.
   * @param confluentPlatformVersion The version of the Confluent Platform this broker is running.
   * @param collectorState The state of the collector (e.g., Running or Shutting down).
   * @param brokerProcessUUID A unique identifier that is valid for the runtime of a broker.  The identifier is generated at broker startup and lost at shutdown/crash.
   * @param clusterId A unique identifier for the Kafka cluster.
   */
  public SupportKafkaMetricsBasic(java.lang.Long timestamp, java.lang.String kafkaVersion, java.lang.String confluentPlatformVersion, java.lang.Integer collectorState, java.lang.String brokerProcessUUID, java.lang.String clusterId) {
    this.timestamp = timestamp;
    this.kafkaVersion = kafkaVersion;
    this.confluentPlatformVersion = confluentPlatformVersion;
    this.collectorState = collectorState;
    this.brokerProcessUUID = brokerProcessUUID;
    this.clusterId = clusterId;
  }

  public org.apache.avro.specific.SpecificData getSpecificData() { return MODEL$; }
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call.
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return timestamp;
    case 1: return kafkaVersion;
    case 2: return confluentPlatformVersion;
    case 3: return collectorState;
    case 4: return brokerProcessUUID;
    case 5: return clusterId;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

  // Used by DatumReader.  Applications should not call.
  @SuppressWarnings(value="unchecked")
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: timestamp = (java.lang.Long)value$; break;
    case 1: kafkaVersion = (java.lang.String)value$; break;
    case 2: confluentPlatformVersion = (java.lang.String)value$; break;
    case 3: collectorState = (java.lang.Integer)value$; break;
    case 4: brokerProcessUUID = (java.lang.String)value$; break;
    case 5: clusterId = (java.lang.String)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }

  /**
   * Gets the value of the 'timestamp' field.
   * @return Time when this data record was created on the broker (Unix time).
   */
  public long getTimestamp() {
    return timestamp;
  }


  /**
   * Sets the value of the 'timestamp' field.
   * Time when this data record was created on the broker (Unix time).
   * @param value the value to set.
   */
  public void setTimestamp(long value) {
    this.timestamp = value;
  }

  /**
   * Gets the value of the 'kafkaVersion' field.
   * @return The version of Kafka this broker is running.
   */
  public java.lang.String getKafkaVersion() {
    return kafkaVersion;
  }


  /**
   * Sets the value of the 'kafkaVersion' field.
   * The version of Kafka this broker is running.
   * @param value the value to set.
   */
  public void setKafkaVersion(java.lang.String value) {
    this.kafkaVersion = value;
  }

  /**
   * Gets the value of the 'confluentPlatformVersion' field.
   * @return The version of the Confluent Platform this broker is running.
   */
  public java.lang.String getConfluentPlatformVersion() {
    return confluentPlatformVersion;
  }


  /**
   * Sets the value of the 'confluentPlatformVersion' field.
   * The version of the Confluent Platform this broker is running.
   * @param value the value to set.
   */
  public void setConfluentPlatformVersion(java.lang.String value) {
    this.confluentPlatformVersion = value;
  }

  /**
   * Gets the value of the 'collectorState' field.
   * @return The state of the collector (e.g., Running or Shutting down).
   */
  public int getCollectorState() {
    return collectorState;
  }


  /**
   * Sets the value of the 'collectorState' field.
   * The state of the collector (e.g., Running or Shutting down).
   * @param value the value to set.
   */
  public void setCollectorState(int value) {
    this.collectorState = value;
  }

  /**
   * Gets the value of the 'brokerProcessUUID' field.
   * @return A unique identifier that is valid for the runtime of a broker.  The identifier is generated at broker startup and lost at shutdown/crash.
   */
  public java.lang.String getBrokerProcessUUID() {
    return brokerProcessUUID;
  }


  /**
   * Sets the value of the 'brokerProcessUUID' field.
   * A unique identifier that is valid for the runtime of a broker.  The identifier is generated at broker startup and lost at shutdown/crash.
   * @param value the value to set.
   */
  public void setBrokerProcessUUID(java.lang.String value) {
    this.brokerProcessUUID = value;
  }

  /**
   * Gets the value of the 'clusterId' field.
   * @return A unique identifier for the Kafka cluster.
   */
  public java.lang.String getClusterId() {
    return clusterId;
  }


  /**
   * Sets the value of the 'clusterId' field.
   * A unique identifier for the Kafka cluster.
   * @param value the value to set.
   */
  public void setClusterId(java.lang.String value) {
    this.clusterId = value;
  }

  /**
   * Creates a new SupportKafkaMetricsBasic RecordBuilder.
   * @return A new SupportKafkaMetricsBasic RecordBuilder
   */
  public static io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder newBuilder() {
    return new io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder();
  }

  /**
   * Creates a new SupportKafkaMetricsBasic RecordBuilder by copying an existing Builder.
   * @param other The existing builder to copy.
   * @return A new SupportKafkaMetricsBasic RecordBuilder
   */
  public static io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder newBuilder(io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder other) {
    if (other == null) {
      return new io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder();
    } else {
      return new io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder(other);
    }
  }

  /**
   * Creates a new SupportKafkaMetricsBasic RecordBuilder by copying an existing SupportKafkaMetricsBasic instance.
   * @param other The existing instance to copy.
   * @return A new SupportKafkaMetricsBasic RecordBuilder
   */
  public static io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder newBuilder(io.confluent.support.metrics.SupportKafkaMetricsBasic other) {
    if (other == null) {
      return new io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder();
    } else {
      return new io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder(other);
    }
  }

  /**
   * RecordBuilder for SupportKafkaMetricsBasic instances.
   */
  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<SupportKafkaMetricsBasic>
    implements org.apache.avro.data.RecordBuilder<SupportKafkaMetricsBasic> {

    /** Time when this data record was created on the broker (Unix time). */
    private long timestamp;
    /** The version of Kafka this broker is running. */
    private java.lang.String kafkaVersion;
    /** The version of the Confluent Platform this broker is running. */
    private java.lang.String confluentPlatformVersion;
    /** The state of the collector (e.g., Running or Shutting down). */
    private int collectorState;
    /** A unique identifier that is valid for the runtime of a broker.  The identifier is generated at broker startup and lost at shutdown/crash. */
    private java.lang.String brokerProcessUUID;
    /** A unique identifier for the Kafka cluster. */
    private java.lang.String clusterId;

    /** Creates a new Builder */
    private Builder() {
      super(SCHEMA$);
    }

    /**
     * Creates a Builder by copying an existing Builder.
     * @param other The existing Builder to copy.
     */
    private Builder(io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder other) {
      super(other);
      if (isValidValue(fields()[0], other.timestamp)) {
        this.timestamp = data().deepCopy(fields()[0].schema(), other.timestamp);
        fieldSetFlags()[0] = other.fieldSetFlags()[0];
      }
      if (isValidValue(fields()[1], other.kafkaVersion)) {
        this.kafkaVersion = data().deepCopy(fields()[1].schema(), other.kafkaVersion);
        fieldSetFlags()[1] = other.fieldSetFlags()[1];
      }
      if (isValidValue(fields()[2], other.confluentPlatformVersion)) {
        this.confluentPlatformVersion = data().deepCopy(fields()[2].schema(), other.confluentPlatformVersion);
        fieldSetFlags()[2] = other.fieldSetFlags()[2];
      }
      if (isValidValue(fields()[3], other.collectorState)) {
        this.collectorState = data().deepCopy(fields()[3].schema(), other.collectorState);
        fieldSetFlags()[3] = other.fieldSetFlags()[3];
      }
      if (isValidValue(fields()[4], other.brokerProcessUUID)) {
        this.brokerProcessUUID = data().deepCopy(fields()[4].schema(), other.brokerProcessUUID);
        fieldSetFlags()[4] = other.fieldSetFlags()[4];
      }
      if (isValidValue(fields()[5], other.clusterId)) {
        this.clusterId = data().deepCopy(fields()[5].schema(), other.clusterId);
        fieldSetFlags()[5] = other.fieldSetFlags()[5];
      }
    }

    /**
     * Creates a Builder by copying an existing SupportKafkaMetricsBasic instance
     * @param other The existing instance to copy.
     */
    private Builder(io.confluent.support.metrics.SupportKafkaMetricsBasic other) {
      super(SCHEMA$);
      if (isValidValue(fields()[0], other.timestamp)) {
        this.timestamp = data().deepCopy(fields()[0].schema(), other.timestamp);
        fieldSetFlags()[0] = true;
      }
      if (isValidValue(fields()[1], other.kafkaVersion)) {
        this.kafkaVersion = data().deepCopy(fields()[1].schema(), other.kafkaVersion);
        fieldSetFlags()[1] = true;
      }
      if (isValidValue(fields()[2], other.confluentPlatformVersion)) {
        this.confluentPlatformVersion = data().deepCopy(fields()[2].schema(), other.confluentPlatformVersion);
        fieldSetFlags()[2] = true;
      }
      if (isValidValue(fields()[3], other.collectorState)) {
        this.collectorState = data().deepCopy(fields()[3].schema(), other.collectorState);
        fieldSetFlags()[3] = true;
      }
      if (isValidValue(fields()[4], other.brokerProcessUUID)) {
        this.brokerProcessUUID = data().deepCopy(fields()[4].schema(), other.brokerProcessUUID);
        fieldSetFlags()[4] = true;
      }
      if (isValidValue(fields()[5], other.clusterId)) {
        this.clusterId = data().deepCopy(fields()[5].schema(), other.clusterId);
        fieldSetFlags()[5] = true;
      }
    }

    /**
      * Gets the value of the 'timestamp' field.
      * Time when this data record was created on the broker (Unix time).
      * @return The value.
      */
    public long getTimestamp() {
      return timestamp;
    }


    /**
      * Sets the value of the 'timestamp' field.
      * Time when this data record was created on the broker (Unix time).
      * @param value The value of 'timestamp'.
      * @return This builder.
      */
    public io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder setTimestamp(long value) {
      validate(fields()[0], value);
      this.timestamp = value;
      fieldSetFlags()[0] = true;
      return this;
    }

    /**
      * Checks whether the 'timestamp' field has been set.
      * Time when this data record was created on the broker (Unix time).
      * @return True if the 'timestamp' field has been set, false otherwise.
      */
    public boolean hasTimestamp() {
      return fieldSetFlags()[0];
    }


    /**
      * Clears the value of the 'timestamp' field.
      * Time when this data record was created on the broker (Unix time).
      * @return This builder.
      */
    public io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder clearTimestamp() {
      fieldSetFlags()[0] = false;
      return this;
    }

    /**
      * Gets the value of the 'kafkaVersion' field.
      * The version of Kafka this broker is running.
      * @return The value.
      */
    public java.lang.String getKafkaVersion() {
      return kafkaVersion;
    }


    /**
      * Sets the value of the 'kafkaVersion' field.
      * The version of Kafka this broker is running.
      * @param value The value of 'kafkaVersion'.
      * @return This builder.
      */
    public io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder setKafkaVersion(java.lang.String value) {
      validate(fields()[1], value);
      this.kafkaVersion = value;
      fieldSetFlags()[1] = true;
      return this;
    }

    /**
      * Checks whether the 'kafkaVersion' field has been set.
      * The version of Kafka this broker is running.
      * @return True if the 'kafkaVersion' field has been set, false otherwise.
      */
    public boolean hasKafkaVersion() {
      return fieldSetFlags()[1];
    }


    /**
      * Clears the value of the 'kafkaVersion' field.
      * The version of Kafka this broker is running.
      * @return This builder.
      */
    public io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder clearKafkaVersion() {
      kafkaVersion = null;
      fieldSetFlags()[1] = false;
      return this;
    }

    /**
      * Gets the value of the 'confluentPlatformVersion' field.
      * The version of the Confluent Platform this broker is running.
      * @return The value.
      */
    public java.lang.String getConfluentPlatformVersion() {
      return confluentPlatformVersion;
    }


    /**
      * Sets the value of the 'confluentPlatformVersion' field.
      * The version of the Confluent Platform this broker is running.
      * @param value The value of 'confluentPlatformVersion'.
      * @return This builder.
      */
    public io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder setConfluentPlatformVersion(java.lang.String value) {
      validate(fields()[2], value);
      this.confluentPlatformVersion = value;
      fieldSetFlags()[2] = true;
      return this;
    }

    /**
      * Checks whether the 'confluentPlatformVersion' field has been set.
      * The version of the Confluent Platform this broker is running.
      * @return True if the 'confluentPlatformVersion' field has been set, false otherwise.
      */
    public boolean hasConfluentPlatformVersion() {
      return fieldSetFlags()[2];
    }


    /**
      * Clears the value of the 'confluentPlatformVersion' field.
      * The version of the Confluent Platform this broker is running.
      * @return This builder.
      */
    public io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder clearConfluentPlatformVersion() {
      confluentPlatformVersion = null;
      fieldSetFlags()[2] = false;
      return this;
    }

    /**
      * Gets the value of the 'collectorState' field.
      * The state of the collector (e.g., Running or Shutting down).
      * @return The value.
      */
    public int getCollectorState() {
      return collectorState;
    }


    /**
      * Sets the value of the 'collectorState' field.
      * The state of the collector (e.g., Running or Shutting down).
      * @param value The value of 'collectorState'.
      * @return This builder.
      */
    public io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder setCollectorState(int value) {
      validate(fields()[3], value);
      this.collectorState = value;
      fieldSetFlags()[3] = true;
      return this;
    }

    /**
      * Checks whether the 'collectorState' field has been set.
      * The state of the collector (e.g., Running or Shutting down).
      * @return True if the 'collectorState' field has been set, false otherwise.
      */
    public boolean hasCollectorState() {
      return fieldSetFlags()[3];
    }


    /**
      * Clears the value of the 'collectorState' field.
      * The state of the collector (e.g., Running or Shutting down).
      * @return This builder.
      */
    public io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder clearCollectorState() {
      fieldSetFlags()[3] = false;
      return this;
    }

    /**
      * Gets the value of the 'brokerProcessUUID' field.
      * A unique identifier that is valid for the runtime of a broker.  The identifier is generated at broker startup and lost at shutdown/crash.
      * @return The value.
      */
    public java.lang.String getBrokerProcessUUID() {
      return brokerProcessUUID;
    }


    /**
      * Sets the value of the 'brokerProcessUUID' field.
      * A unique identifier that is valid for the runtime of a broker.  The identifier is generated at broker startup and lost at shutdown/crash.
      * @param value The value of 'brokerProcessUUID'.
      * @return This builder.
      */
    public io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder setBrokerProcessUUID(java.lang.String value) {
      validate(fields()[4], value);
      this.brokerProcessUUID = value;
      fieldSetFlags()[4] = true;
      return this;
    }

    /**
      * Checks whether the 'brokerProcessUUID' field has been set.
      * A unique identifier that is valid for the runtime of a broker.  The identifier is generated at broker startup and lost at shutdown/crash.
      * @return True if the 'brokerProcessUUID' field has been set, false otherwise.
      */
    public boolean hasBrokerProcessUUID() {
      return fieldSetFlags()[4];
    }


    /**
      * Clears the value of the 'brokerProcessUUID' field.
      * A unique identifier that is valid for the runtime of a broker.  The identifier is generated at broker startup and lost at shutdown/crash.
      * @return This builder.
      */
    public io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder clearBrokerProcessUUID() {
      brokerProcessUUID = null;
      fieldSetFlags()[4] = false;
      return this;
    }

    /**
      * Gets the value of the 'clusterId' field.
      * A unique identifier for the Kafka cluster.
      * @return The value.
      */
    public java.lang.String getClusterId() {
      return clusterId;
    }


    /**
      * Sets the value of the 'clusterId' field.
      * A unique identifier for the Kafka cluster.
      * @param value The value of 'clusterId'.
      * @return This builder.
      */
    public io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder setClusterId(java.lang.String value) {
      validate(fields()[5], value);
      this.clusterId = value;
      fieldSetFlags()[5] = true;
      return this;
    }

    /**
      * Checks whether the 'clusterId' field has been set.
      * A unique identifier for the Kafka cluster.
      * @return True if the 'clusterId' field has been set, false otherwise.
      */
    public boolean hasClusterId() {
      return fieldSetFlags()[5];
    }


    /**
      * Clears the value of the 'clusterId' field.
      * A unique identifier for the Kafka cluster.
      * @return This builder.
      */
    public io.confluent.support.metrics.SupportKafkaMetricsBasic.Builder clearClusterId() {
      clusterId = null;
      fieldSetFlags()[5] = false;
      return this;
    }

    @Override
    @SuppressWarnings("unchecked")
    public SupportKafkaMetricsBasic build() {
      try {
        SupportKafkaMetricsBasic record = new SupportKafkaMetricsBasic();
        record.timestamp = fieldSetFlags()[0] ? this.timestamp : (java.lang.Long) defaultValue(fields()[0]);
        record.kafkaVersion = fieldSetFlags()[1] ? this.kafkaVersion : (java.lang.String) defaultValue(fields()[1]);
        record.confluentPlatformVersion = fieldSetFlags()[2] ? this.confluentPlatformVersion : (java.lang.String) defaultValue(fields()[2]);
        record.collectorState = fieldSetFlags()[3] ? this.collectorState : (java.lang.Integer) defaultValue(fields()[3]);
        record.brokerProcessUUID = fieldSetFlags()[4] ? this.brokerProcessUUID : (java.lang.String) defaultValue(fields()[4]);
        record.clusterId = fieldSetFlags()[5] ? this.clusterId : (java.lang.String) defaultValue(fields()[5]);
        return record;
      } catch (org.apache.avro.AvroMissingFieldException e) {
        throw e;
      } catch (java.lang.Exception e) {
        throw new org.apache.avro.AvroRuntimeException(e);
      }
    }
  }

  @SuppressWarnings("unchecked")
  private static final org.apache.avro.io.DatumWriter<SupportKafkaMetricsBasic>
    WRITER$ = (org.apache.avro.io.DatumWriter<SupportKafkaMetricsBasic>)MODEL$.createDatumWriter(SCHEMA$);

  @Override public void writeExternal(java.io.ObjectOutput out)
    throws java.io.IOException {
    WRITER$.write(this, SpecificData.getEncoder(out));
  }

  @SuppressWarnings("unchecked")
  private static final org.apache.avro.io.DatumReader<SupportKafkaMetricsBasic>
    READER$ = (org.apache.avro.io.DatumReader<SupportKafkaMetricsBasic>)MODEL$.createDatumReader(SCHEMA$);

  @Override public void readExternal(java.io.ObjectInput in)
    throws java.io.IOException {
    READER$.read(this, SpecificData.getDecoder(in));
  }

  @Override protected boolean hasCustomCoders() { return true; }

  @Override public void customEncode(org.apache.avro.io.Encoder out)
    throws java.io.IOException
  {
    out.writeLong(this.timestamp);

    out.writeString(this.kafkaVersion);

    if (this.confluentPlatformVersion == null) {
      out.writeIndex(0);
      out.writeNull();
    } else {
      out.writeIndex(1);
      out.writeString(this.confluentPlatformVersion);
    }

    out.writeInt(this.collectorState);

    out.writeString(this.brokerProcessUUID);

    out.writeString(this.clusterId);

  }

  @Override public void customDecode(org.apache.avro.io.ResolvingDecoder in)
    throws java.io.IOException
  {
    org.apache.avro.Schema.Field[] fieldOrder = in.readFieldOrderIfDiff();
    if (fieldOrder == null) {
      this.timestamp = in.readLong();

      this.kafkaVersion = in.readString();

      if (in.readIndex() != 1) {
        in.readNull();
        this.confluentPlatformVersion = null;
      } else {
        this.confluentPlatformVersion = in.readString();
      }

      this.collectorState = in.readInt();

      this.brokerProcessUUID = in.readString();

      this.clusterId = in.readString();

    } else {
      for (int i = 0; i < 6; i++) {
        switch (fieldOrder[i].pos()) {
        case 0:
          this.timestamp = in.readLong();
          break;

        case 1:
          this.kafkaVersion = in.readString();
          break;

        case 2:
          if (in.readIndex() != 1) {
            in.readNull();
            this.confluentPlatformVersion = null;
          } else {
            this.confluentPlatformVersion = in.readString();
          }
          break;

        case 3:
          this.collectorState = in.readInt();
          break;

        case 4:
          this.brokerProcessUUID = in.readString();
          break;

        case 5:
          this.clusterId = in.readString();
          break;

        default:
          throw new java.io.IOException("Corrupt ResolvingDecoder.");
        }
      }
    }
  }
}










